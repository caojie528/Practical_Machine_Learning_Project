---
title: "Human Activity Prediction"
author: "Jie Cao"
date: "12/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, I will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. We will build a machine learning model to predict the activity they performed.

More information about the Weight Lifting Exercise Dataset is available from the [website](http://groupware.les.inf.puc-rio.br/har). 

## Libraries

```{r libraries, message=FALSE, warning=FALSE}
library(caret)
library(rattle)
```

## Data Loading

```{r load_data}
# Download the data
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "./data/pml-training.csv"
testFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {
  dir.create("./data")
}
if (!file.exists(trainFile)) {
  download.file(trainURL, destfile=trainFile, method="curl")
}
if (!file.exists(testFile)) {
  download.file(testURL, destfile=testFile, method="curl")
}
# Load data
trainRaw <- read.csv("./data/pml-training.csv")
testRaw <- read.csv("./data/pml-testing.csv")

# Explore data
dim(trainRaw)
dim(testRaw)
```

The training data has `r dim(trainRaw)[1]` observations, and the testing data has `r dim(testRaw)[1]` observations. They each has 160 variables.

## Data Processing

```{r data_str}
str(trainRaw)
```

We notice that some variables contain NA values. We remove them from the dataset.

```{r remove_na}
trainData <- trainRaw[, colSums(is.na(trainRaw)) == 0]
testData <- testRaw[, colSums(is.na(testRaw)) == 0]
dim(trainData)
dim(testData)
```

There are also some variables we suspect providing very few information in prediction - first 7 columns (participant IDs, timestamps and windows). 

```{r remove_7vars}
trainData <- trainData[, -c(1:7)]
testData <- testData[, -c(1:7)]
dim(trainData)
dim(testData)
```

Now we keep variables that exist in both training and testing, and check if there's remaining variables of near zero variance. 

```{r remove_nzv}
trainVars <- names(trainData)
testVars <- names(testData)
bothVars <- intersect(trainVars, testVars)

trainData <- trainData[, c(bothVars, "classe")]
testData <- testData[, bothVars]

NZV <- nearZeroVar(trainData)
length(NZV)

dim(trainData)
dim(testData)
```

After cleaning, the training data has 53 variables (52 potential predictors and 1 label) and the testing data has 52 variables (52 potential predictors). 

## Modeling

We will fit three different models in this section. They are **classification tree**, **random forest** and **gradient boosting machine**.

### Data split

In order to evaluate models, we further split the cleaned training data into training and validation sets.

```{r train_val_split}
set.seed(1)
inTrain <- createDataPartition(trainData$classe, p = 0.70, list = FALSE)
train <- trainData[inTrain, ]
validation <- trainData[-inTrain, ]
```

### Classification tree

To choose the best model and avoid overfitting, we use 5-fold cross-validation to find the best classification tree model. 

```{r cart_train}
model_ct <- train(classe ~ ., data = train, method = "rpart", 
                  trControl = trainControl(method = "cv", number = 5))
fancyRpartPlot(model_ct$finalModel)
```

```{r cart_validation}
val_ct <- predict(model_ct, newdata = validation)
conf_mat_ct <- confusionMatrix(validation$classe, val_ct)
conf_mat_ct$table
conf_mat_ct$overall[1]
```

The selected classification tree only achieve an accuracy of `r round(conf_mat_ct$overall[1], 2)` in the validation set. 

### Random forest

```{r rf_train, cache = TRUE}
model_rf <- train(classe ~ ., data = train, method = "rf",  
                  trControl = trainControl(method = "cv", number = 5), 
                  verbose = FALSE)
model_rf
```

The best random forest model selected by the cross-validation uses 27 variable at each split. Below we show the variable importance in the model. We notice that `roll_belt`, `pitch_forarm`, `yaw_belt`, `magnet_dumbell_y` and `pitch_belt` are the top 5 important variables in this model. 

```{r rf_varimp}
plot(varImp(model_rf))
```


```{r rf_validation}
val_rf <- predict(model_rf, newdata = validation)
conf_mat_rf <- confusionMatrix(validation$classe, val_rf)
conf_mat_rf$table
conf_mat_rf$overall[1]
```

The selected random forest model can achieve an accuracy of `r round(conf_mat_rf$overall[1], 2)` in the validation set. 

### Gradient boosting machine

```{r gbm_train, cache = TRUE}
model_gbm <- train(classe ~ ., data = train, method = "gbm", 
                   trControl = trainControl(method = "cv", number = 5), 
                   verbose = FALSE)
model_gbm
```

The final model selected by the cross-validation uses 150 trees, depth of 3 for each tree, shrinkage of 0.1 and minimum obervations in each node of 10. 

```{r gbm_validation}
val_gbm <- predict(model_gbm, newdata = validation)
conf_mat_gbm <- confusionMatrix(validation$classe, val_gbm)
conf_mat_gbm$table
conf_mat_gbm$overall[1]
```

The selected random forest model can achieve an accuracy of `r round(conf_mat_gbm$overall[1], 2)` in the validation set. 

## Prediction

Based on the results we obtained above, the random forest model is the best one in terms of validation accuracy. Hence, we use the random forest model to make prediction on the test dataset. 

```{r rf_test}
test_rf <- predict(model_rf, newdata = testData)
test_rf
```


